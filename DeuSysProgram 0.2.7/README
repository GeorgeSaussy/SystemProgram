README


Introduction
This directory should contain all the files and programs necessary to quickly
replicate the results documented in the write up. All files should be
self-documented in addition to an outline of their use below. Each of the
computer experiments run will be documented separately. All binaries were
compiled and ran on Ubuntu 14. They may need to be recompiled for another OS.
The documentation for the data files is sparse.

The data
All of the data was taken from "spiegel.de". My understanding is that the site
is a German mirror of Project Gutenburg. The data was downloaded by python glue
scripts using the Linux terminal browser lynx with the flag "--dump". The
directories for each author are located in file in "rawdata" carrying the
respective authors names. The original files downloaded were file linking to
each of works by each of the respective authors. Those links were then
downloaded in the same way and were then parsed for the text of each work. These
parses are the "*corpus" and "*corpus1" files in the main directory. Both the
downloading and the parsing was imperfect and introduced errors in the
transcription. Steps were taken to clean up the data, but I suspect error remain
and steps were taken to correct for these. (The corpora could not be reviewed by
hand in time for the project's deadline.) This is discussed in the writeup and
some of the documentation.


Naive Bayes
The Naive Bayes is run by, in the naivebayes directory, running first
	$ python naivebayes_logwriter.py -d func
and then
	$ python naivebayse_classifier.py
As an example "logfreq_data" contains the printed information from
naivebayes_logwriter.py. The model prints the log-likelihood that each author
wrote the text. It also prints the confusion matrix given the model only trained
on 90% of the data and tested on the other 10%. The model uses +1 smoothing. See
the "naivebayes_logwriter.py" and "naivebayes_classifier.py" self-documentation
for further details.
In addition, if instead of with the "func" option, one writes
    $ python naivebayes_logwriter.py -d all
the program will use the authors' combined vocabulary to run the Naive Bayes
model instead of the just the functional words. See the writeup for more notes
on the distinction. Binning changes the results and is discussed in the writeup.
See "naivebayes/README" for further documentation. Both
"naivebayes_logwriter.py" and "naivebayes_classifier.py" are self documented as
well.


SVM
In the directory "svm" is a directory "libsvm-master". This directory is a copy
of the LIBSVM suite by Chih-Chung Chang and Chih-Jen Lin. (Some documentation
for the software can be found at csie.ntu.edu.tw/~cjlin/libsvm/ and the GitHub
repo for the software can be found at github.com/cjlin1/libsvm .) To run the
SVM, cd into the svm directory. There, run first
    $ python svm_data_writer.py
Then run
    $ libsvm-master/svm-train -s 0 -t 1 -d 2 all_svm_data all_svm_data_model
and finally
    $  libsvm-master/svm-predict sysprogram_svm_data all_svm_data_model sysprogram_svm_prediction
The file "sysprogram_svm_prediction" now contains the output of the SVM's
prediction. 1 corresponds to Hegel, 2 to Hölderlin, and  o Schelling.
To replicate the testing of the SVM model, call
    $ libsvm-master/svm-train -s 0 -t 1 -d 2 training_svm_data training_svm_data_model
and then
    $ libsvm-master/svm-predict testing_svm_data training_svm_data_model testing_svm_prediction
If everything went well, you should get an accuracy of 55.2083%. Reviewing
testing_svm_prediction should reveal every datum was classified as 2, or as by
Hölderlin. This indicated the data was not seperable. The failure of this model
is discussed in the writeup.
See "svm/README" for further documentation. "svm_data_writer.py" is
self-documented as well. For further documentation on LIBSVM,
"svm/libsvm-master/README" has an outline. In addition, calling any of the
LIBSVM binaries without options will print help.


Perceptron
The perceptron is run by, in the "perceptron" sub-directory using the command
    $ python perceptron.py --eta [float] --itrs [int]
Where the "eta" parameter is the learning factor for the model and the "itrs"
parameter is the number of iterations. The program will print the strength with
which a perceptron trained to recognize each author believes the System Program
was written by that author. It was found that the model was extremely sensitive
to the learning parameter. This is consistent with the results from the SVM and
indicate that the data in the training corpus is not separable. This is further
discussed in the writeup. See "perceptron/README" for further documentation and
"perceptron.py" is self-documented as well.
